{Miller2018}
we need a model big enough and fast enough to predict structure - uses OPLS-UA
order parameter is used to quantify structural order and is related to the lamellar stacking in p3ht--when systems are small GIXS peaks are fuzzy.
relaxation time and TPS
    these metrics were used to show that advances in hardware wouldn't give us the jump we needed and so we need better models
    still it would be cool to compare
decorrelation times/autocorrelation - used to make sure the the system is equilibrated
Conclusions:
    Can simulate solvent evaporation with slow shrinking
    large volumes are needed for experimental validation

planckton removing hydrogens doesn't adjust the mass of heavy atoms - does this matter?

I found that in order to get agreement with Evan's results I had to change the angle cutoff to be more strict.

GRiTS
uses SMARTS matching to assign atomic indexes to coarse grain beads
bonds between cg beads are inferred based on atomistic bonding scheme
SMARTS matching is expensive so when looking at an entire simulation trajectory a simplification scheme is used
first the first frame of the trajectory is taken
this is because the perceive bond order function in openbabel relies on the atomic positions being chemically reasonable
(aromatic systems should be planar, etc)
freud clustering is used to break apart the first frame into molecules
if molecules have the same number of atoms, they are assumed to have the same chemical structure

Next steps
Evan's work showed that tie-chains are important how does the planckton workflow address the need for polydispersity
Opportunity to cite Chris' polymer builder work!

How have we made this work more TRUE?
T: by using mbuild with foyer in planckton to initialize our simulations, we can easily use different input file formats including smiles strings and any foyer forcefield. Using grits to calculate the order parameter can help us to more easily select the conjugated part of the molecule without any manual indexing.
R: by using the signac framework we can quickly and easily sample the necessary parameter space without needing to manually create or manage directories. We've also implemented semantic versioning in planckton with version tagged docker containers which helps to get the same code state.
U: PEP8 compliant code and docstrings are ensured by using the pre-commit git hook manager to automatically lint the code with black, isort, pydocstyle. Along with jupyter notebook examples, Grits also has a sphinx readthedocs page.
E: by building on existing code we work with a community of open-source molecular simulators. <Mention Chris' work with uli and the polymer builder? The polymer builder could be a great way to entend planckton>


Outline
Intro
    OPVS - meet global energy needs while not contributing to CO2 emissions
        low manufacturing costs shorter energy payback times 
    Simulations can help us to better understand what compounds and processing consitions result in the best structure
    This paper will demonstrate that we can reproduce and validate our p3ht morphologies while using mosdef tools to help our methods to be more TRUE

what is the goal? / statement of need
    quickly and easily spin up a large number of simulations
    access each of those simulations within that parameter space
    perform analysis on the simulation output that involves picking out a chemical structure

TRUE challenges
    when I first started in the lab installing our software stack was difficult. Managing dependencies between mbuild and foyer required that we install the dependencies of both then in a separate step install the actual packages. We also had to compile hoomd from source if we wanted it to work on GPU. Now with improvements in their conda packaging and by using the mamba package manager all packages can be installed in one step. But in the interim we set up a pipeline which automatically builds docker containers from each tagged version or the laster master branch. This allows our labmates to use this package without ever having to build the software stack. Singularity is installed on all our clusters and so we can simply pull our singularity image and run inside the image. Not only does this make our software stack easier to use, but it makes it more portable between clusters--we can have the exact same environment on our schools cluster and on XSEDE--and it makes it reproducible--if someone wants to reproduce our work, they can access the exact container we used.
    manual atom typing
    filling the box using mbuild vs another program -- switching between programs can introduce error
    having the entire workflow scripted reduces oportunity for error

Tools
    planckton
    grits
    gixstapose
- structure idea:
    overall description
    chapter about each tool
    scientific application of tools

Notes from some AIChE talks:
    Justin's:
    "Reproducible computational research, in which all details of computations--code and data--are made conveniently available to others, is a necessary response to [the credibility] crisis" - Donoho, D. et. al. (2009) Comp Sci Eng 11(1)
    "Automation and version control are key devices for reproducibility" - Sandve, G. K. (2013) PLOS Comp Bio 9

    Dr Phelan:
    Something that stands out to me about this talk is the amount of data which is carried between different scripts. I wonder if this process is automated? Also no mention of how the mapping is generated. Also no code is distributed--is it public or version controlled? how is lammps integrated into the workflow or is this done manually?
    where can comsoft be found? 
    they have a coarse grain mapping routine which appears to use regex/graph network/smiles
    - t khanh hoang
